## 一.学习内容

- 完成多元线性回归的类封装
  - 代码实现解析解求解和梯度下降法求解，**参考sklearn接口形式** ，不允许调用机器学习库，仅可使用numpy以波士顿房价数据集为例，划分训练集，测试集，并完成数据分析和预处理，完成模型训练并予以评价分析结果

## 二.学习笔记

### 1.安装机器学习库

~~~
C:\Users\EC319>pip install --target=C:\Users\EC319\AppData\Local\Programs\Python\Python312\Lib\site-packages scikit-learn
~~~

### 2.代码分析

#### （1）载入加利福尼亚房价数据集

因为安装的sklearn版本没有波士顿房价的数据集了...

~~~python
def load_boston_data():
    from sklearn.datasets import load_boston
    data = fetch_california_housing()
    X, y = data.data, data.target
    return X, y
~~~

存入特征矩阵X，和目标向量y并返回

#### （2）预处理数据

~~~python
def preprocess_data(X, y, test_size=0.2, random_state=10000):
    # 随机排列
    np.random.seed(random_state)
    X = np.concatenate([np.ones((X.shape[0], 1)), X], axis=1)
    indices = np.random.permutation(X.shape[0])
    # 划分训练集测试集
    test_size = int(X.shape[0] * test_size)
    X_train, X_test = X[indices[:-test_size]], X[indices[-test_size:]]
    y_train, y_test = y[indices[:-test_size]], y[indices[-test_size:]]
    return X_train, X_test, y_train, y_test
~~~

##### [1] 划分截距项

~~~python
X = np.concatenate([np.ones((X.shape[0], 1)), X], axis=1)
~~~

在特征矩阵 `X` 的左侧添加一列全为1的列向量，以表示截距项

- `np.ones((X.shape[0], 1))` 创建了一个形状为 `(X.shape[0], 1)` 的全为1的矩阵，表示截距项
- `np.concatenate([np.ones((X.shape[0], 1)), X], axis=1)` 将截距项矩阵和特征矩阵 `X` 沿着列的方向进行拼接，得到一个新的特征矩阵 `X`，其中第一列为全为1的截距项列，其余列为原始特征列。

##### [2] 划分训练集和测试集

~~~python
indices = np.random.permutation(X.shape[0])
test_size = int(X.shape[0] * test_size)
X_train, X_test = X[indices[:-test_size]], X[indices[-test_size:]]
y_train, y_test = y[indices[:-test_size]], y[indices[-test_size:]]
return X_train, X_test, y_train, y_test
~~~

将数据集划分为训练集和测试集，并返回划分好的特征矩阵和目标向量

###### ① 打乱数据集顺序

~~~python
indices = np.random.permutation(X.shape[0])
~~~

`np.random.permutation(X.shape[0])` 生成了一个随机排列的长度为 `X.shape[0]` 的索引数组，用来打乱数据集的顺序

- 所生成的是一个长度为 `X.shape[0]` 的随机排列数组，即生成一个包含从 0 到 `X.shape[0]-1` 的整数的随机排列

- 函数原理：在给定的范围内生成一个随机的排列，并且这个排列是均匀分布的，每个元素的概率都是相同的。这种实现方式保证了生成的随机排列具有随机性和均匀性，可以很好地用来打乱数据集的顺序

  - 示例：

    当 `X.shape[0]` 为 5，即数据集包含 5 个样本时，我们可以通过 `np.random.permutation(X.shape[0])` 生成一个随机排列数组，然后利用这个随机排列数组对数据集进行打乱

    假设我们有一个数据集 `X`，其中包含 5 个样本，每个样本有 3 个特征。数据集 `X` 如下所示：

    ```
    X = [[1, 2, 3],
         [4, 5, 6],
         [7, 8, 9],
         [10, 11, 12],
         [13, 14, 15]]
    ```

    首先，我们生成一个随机排列数组 `indices`：

    ```python
    import numpy as np
    
    # 数据集包含 5 个样本
    X = np.array([[1, 2, 3],
                  [4, 5, 6],
                  [7, 8, 9],
                  [10, 11, 12],
                  [13, 14, 15]])
    
    # 生成随机排列数组
    indices = np.random.permutation(X.shape[0])
    print("随机排列数组:", indices)
    ```

    假设生成的随机排列数组为 `[2, 4, 1, 3, 0]`。接着，我们使用这个随机排列数组对数据集进行打乱，得到打乱后的数据集：

    ```python
    # 打乱数据集
    shuffled_X = X[indices]
    print("打乱后的数据集:\n", shuffled_X)
    ```

    打印结果：

    ```
    随机排列数组: [2 4 1 3 0]
    打乱后的数据集:
     [[ 7  8  9]
     [13 14 15]
     [ 4  5  6]
     [10 11 12]
     [ 1  2  3]]
    ```

    可以看到，原始数据集 `X` 的顺序被随机打乱了，并且顺序是根据生成的随机排列数组确定的。

###### ② 计算测试集大小

~~~python
test_size = int(X.shape[0] * test_size)
~~~

- 计算测试集的大小，取数据集总样本数的 `test_size` 比例作为测试集的大小
  - `X.shape[0]` 表示数据集 `X` 中样本的数量，即数据集的总样本数。
  - `test_size` 参数是自主设定的一个介于 0 和 1 之间的浮点数，表示测试集在整个数据集中所占的比例，即 `test_size` 设置为 0.2，则测试集将占据整个数据集的 20%
    - 示例：若数据集 `X` 包含了 100 个样本，而 `test_size` 参数设置为 0.2，即测试集占总样本数的 20%。那么通过计算 `int(X.shape[0] * test_size)`，得到的结果就是 20。因此，测试集的大小为 20 个样本。

###### ③ 取出训练集和测试集

~~~python
X_train, X_test = X[indices[:-test_size]], X[indices[-test_size:]]
y_train, y_test = y[indices[:-test_size]], y[indices[-test_size:]]
~~~

- `X[indices[:-test_size]]` 通过随机排列的索引数组，取出除了最后 `test_size` 个样本外的所有样本作为训练集。`X[indices[-test_size:]]` 取出最后 `test_size` 个样本作为测试集。

- `y[indices[:-test_size]]` 和 `y[indices[-test_size:]]` 对目标向量进行相同的划分操作，保证特征矩阵和目标向量的对应关系不变。

#### （4）线性回归模型的实现

~~~python
class LinearRegression:
    def __init__(self):
        self.mdl_ = None

    def fit_analytical(self, X, y):
        self.mdl_ = np.linalg.inv(X.T @ X) @ X.T @ y

    def fit_gradient_descent(self, X, y, learning_rate=0.001, n_iterations=1000):
        n_samples, n_features = X.shape
        self.mdl_ = np.zeros(n_features)

        for _ in range(n_iterations):
            gradients = -(2 / n_samples) * X.T @ (y - X @ self.mdl_)
            self.mdl_ -= learning_rate * gradients

    def predict(self, X):
        return X @ self.mdl_
~~~

##### ① 初始化

~~~python
def __init__(self):
        self.mdl_ = None
~~~

- 初始化模型对象，将模型存储在系数`self.mdl_`中

##### ② 解析解

使用解析解方法计算线性回归模型的系数，即使用矩阵运算求解线性方程组的解

~~~python
def fit_analytical(self, X, y):
        self.mdl_ = np.linalg.inv(X.T @ X) @ X.T @ y
~~~

- `np.linalg.inv()` 函数用于计算矩阵的逆，`@` 符号用于执行矩阵乘法操作。
- `np.linalg.inv(X.T @ X)`首先计算特征矩阵 `X` 的转置矩阵 `X.T` 与自身矩阵相乘的结果，得到一个方阵，然后调用 `np.linalg.inv()` 函数计算这个方阵的逆矩阵。这个步骤用于计算线性回归模型的正规方程解。
- 最后一步 `np.linalg.inv(X.T @ X) @ X.T @ y` 将以上步骤的结果相乘，得到线性回归模型的系数向量 `self.mdl_`，这个系数向量是使得线性回归模型最小化残差平方和的系数。

##### ③ 梯度下降法

```python
def fit_gradient_descent(self, X, y, learning_rate=0.001, n_iterations=1000):
    n_samples, n_features = X.shape
    self.mdl_ = np.zeros(n_features)

    for _ in range(n_iterations):
        gradients = -(2 / n_samples) * X.T @ (y - X @ self.mdl_)
        self.mdl_ -= learning_rate * gradients
```
- `learning_rate` 是学习率，控制每次更新的步长，默认为0.001。
- `n_iterations` 是迭代次数，即进行梯度下降的次数，默认为1000次。

- ~~~python
  n_samples, n_features = X.shape
  ~~~

  `X.shape` 返回一个元组，其中包含矩阵 `X` 的形状信息。对于二维矩阵 `X`，`X.shape` 的返回值是一个包含两个元素的元组，第一个元素表示矩阵的行数（样本数），第二个元素表示矩阵的列数（特征数）。因此，可以使用多重赋值语法将 `X.shape` 的返回值分别赋给 `n_samples` 和 `n_features`，以获取矩阵的行数和列数。

- ~~~python
  self.mdl_ = np.zeros(n_features)
  ~~~

  初始化模型系数向量为一个全零向量，长度为特征数 `n_features`。这个向量用于存储模型的系数

- ~~~python
  for k in range(n_iterations):
          gradients = -(2 / n_samples) * X.T @ (y - X @ self.mdl_)
          self.mdl_ -= learning_rate * gradients
  ~~~

  - `for _ in range(n_iterations)`：进行指定次数的梯度下降迭代
  - `gradients = -(2 / n_samples) * X.T @ (y - X @ self.coef_)`：计算梯度，即目标函数关于系数向量的偏导数。这里使用了梯度下降的公式，其中 `(y - X @ self.coef_)` 计算了模型预测值与真实值之间的误差，然后乘以特征矩阵 `X` 的转置 `X.T`，再除以样本数 `n_samples`。
  - `self.coef_ -= learning_rate * gradients`：根据学习率和梯度更新系数向量。学习率 `learning_rate` 控制了每次更新的步长。通过将学习率乘以梯度，得到了每个系数的更新量，然后将更新量从当前的系数向量中减去，从而更新系数向量。

##### ④ 预测

~~~python
def predict(self, X):
        return X @ self.mdl_
~~~

这里的 `self.coef_` 包含了线性回归模型的系数，即模型的参数。通过将输入特征矩阵与系数向量相乘，得到预测值。（预测值是通过将输入特征与对应系数相乘后求和得到的，这是线性回归模型的基本形式。）

#### （5）正式训练

##### ① 加载数据

~~~python
X, y = load_california_data()
~~~

##### ② 划分测试集和训练集

~~~python
X_train, X_test, y_train, y_test = preprocess_data(X, y)
~~~

##### ③ 实例化模型

~~~python
model_analytical = LinearRegression()
model_gradient_descent = LinearRegression()
~~~

##### ④ 解析解和梯度下降

~~~python
model_analytical.fit_analytical(X_train, y_train)
model_gradient_descent.fit_gradient_descent(X_train, y_train)
~~~

##### ⑤ 测试集上进行预测

~~~python
y_pred_analytical = model_analytical.predict(X_test)
y_pred_gradient_descent = model_gradient_descent.predict(X_test)
~~~

##### ⑥ 评价模型

~~~python
mse_analytical = np.mean((y_test - y_pred_analytical) ** 2)
mse_gradient_descent = np.mean((y_test - y_pred_gradient_descent) ** 2)

print("MSE (Analytical Solution):", mse_analytical)
print("MSE (Gradient Descent):", mse_gradient_descent)
~~~

通常情况下，MSE越小，表示模型的预测效果越好。