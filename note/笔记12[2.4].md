## 梯度下降算法

### 一.学习内容

* 多元线性回归梯度下降法求解
  * 梯度下降法
* 多元线性回归解析解
  * 最小二乘法构建损失函数 求矩阵与向量偏导并手推公式

### 二.学习笔记

#### 1.线性回归原理

##### （1）定义

* 线性

  * 可加性：$f(x+y) = f(x) + f(y)$

  * 齐次性：$f(ax) = af(x)$，其中a为与x无关的常数
  * 描述线性

$$
f(x, y) = f(ax+by) = af(x)+bf(y)
$$

* 回归
  * 确定多个变量间相互依赖的定量关系

##### （2）损失函数

###### ①最小化损失函数

假设要拟合一条一次线性方程 $y=kx+b$ ：

使用均方误差MSE作为损失函数，是关于变量k，b的函数
$$
L(k,b) = \frac{1}{m} \sum_{i=1}^m((kx_{i}+b)-y_{i})^2
$$
其中m为样本个数，此时要最小化$L(k,b)$，所以求导，导数等于0时的x让函数取得最小值

两个变量，则求偏导，找偏导等于0

最小二乘法：

* 求b的偏导

$$
\sum_{i=1}^m2(kx_{i}+b-y_{i}) = 0 \\
k\sum_{i=1}^mx_{i}+\sum_{i=1}^mb - \sum_{i=1}^my_{i} = 0 \\
km \bar{x} + mb - m \bar{y} = 0 \\
b = \bar{y} - k \bar{x} \quad ①
$$

* 求k的偏导

$$
\sum_{i=1}^mx_{i}(kx_{i}+b-y_{i}) = 0 \\
k\sum_{i=1}^mx_{i}^2 + b\sum_{i=1}^mx_{i} - \sum_{i=1}^mx_{i}y_{i} = 0 \\
k\sum_{i=1}^mx_{i}^2 + mb \bar{x} - \sum_{i=1}^mx_{i}y_{i} = 0 \quad ②\\ 
* \bar{x} = km\bar{x}^2 + mb\bar{x} - m \bar{y} \bar{x} = 0 \quad ③ \\
②-③： \\
k(\sum_{i=1}^mx_{i}^2-m\bar{x}^2)=\sum_{i=1}^mx_{i}y_{i} - m\bar{y} \bar{x} \\
k = \frac{\sum_{i=1}^{m}x_{i}y_{i}- m\bar{x} \bar{y}}{\sum_{i=1}^mx_{i}^2-m\bar{x}^2}
$$

推广到多元变量，多元函数式为：
$$
f(x_1, x_2, \dots, x_n) = w_1x_1+w_2x_2+\dots+w_nx_n+b
$$
使用线性代数的向量概念整理，记$w_0=b$，默认为列向量，带T为行向量
$$
w = (w_0, w_1, \dots, w_n) \\
x = (1, x_1, x_2, \dots, x_n) \\
L(w, x) = \sum_{i=1}^{m}(y_i-w^Tx^{(i)})^2
$$
$y_i$为第$i$个真实值，$x^{(i)}$为第$i$个样本的特征向量

$$
\frac {\partial L (w)}{\partial w} = 
\frac {\partial (w^TX^TXw-2w^TX^Ty-y^Ty)}{\partial w} \\
=\frac{\partial w^TX^TXw}{\partial w} - 2 *\frac{\partial w^TX^Ty}{\partial w} - 0\\
=X^TXw+X^TXw - 2X^Ty \\
= 2X^T(Xw-y)
$$

##### ==（3）实际代码==

~~~python
def fit_analytical(self, X, y):
        self.mdl_ = np.linalg.inv(X.T @ X) @ X.T @ y
~~~

`X` 是输入特征矩阵，每一行代表一个样本，每一列代表一个特征；`y` 是对应的目标值向量。具体的数学原理如下：

1. 我们的线性模型可以表示为：
   $$
   y = X \cdot \theta
   $$
   其中，$X$ 是输入特征矩阵，$\theta$ 是模型参数向量，$y$ 是目标值向量。

2. 我们的目标是找到最优的模型参数 $\theta$，使得模型能够最好地拟合数据。为了衡量模型的拟合程度，通常使用最小化平方误差（Mean Squared Error, MSE）作为损失函数，即：
   $$
   \text{MSE}(\theta) = \frac{1}{n} \sum_{i=1}^{n} (y_i - X_i \cdot \theta)^2
   $$
   其中，$n$ 是样本数量。

3. 为了找到最小化 MSE 的模型参数 $\theta$，我们可以对 MSE 关于 $\theta$ 的求导，然后令导数等于零求解。这样得到的解称为解析解，即解析地通过数学公式计算得到的模型参数。

4. 根据最小二乘法的公式，我们可以得到解析解为：
   $$
   \theta = (X^T X)^{-1} X^T y
   其中，\((X^T X)^{-1}\) 是输入特征矩阵 \(X\) 的转置矩阵与自身的乘积的逆矩阵。
   $$
   其中，$(X^T X)^{-1}$ 是输入特征矩阵 $X$ 的转置矩阵与自身的乘积的逆矩阵。

### 2.梯度下降法

##### （1）梯度向量

* 指向函数值增长最快的方向
* 如果是负的梯度向量，则是指向函数值减少最快的方向

以 $f(x)=x^2$ 为例：

它的梯度向量为
$$
\nabla f(x_p) = f'(x_p)i = (2x|_{x=x_p}) = 2x_p
$$
假设起点在$x_0=10$处，将球在此释放，它此时的梯度向量为$2x_p=2*10=20$

还需要一个描述球滚动的快慢程度，叫**步长$\alpha$**（也叫学习率）

将此节的$\alpha$设为0.2，则
$$
x_1 = x_0 - \alpha \nabla f(x_0) = 10 - 0.2*20 = 6
$$
小球下降到了$x_1 = 6$的位置

![](https://sevanthea7.oss-cn-beijing.aliyuncs.com/QGworks/202402051117236.png)

然后用同样的方法操作$x_1$得到$x_2$，以此类推

![](https://sevanthea7.oss-cn-beijing.aliyuncs.com/QGworks/202402051121807.png)

通过求梯度和设定学习率经过迭代最终达到最小值点，求出损失函数对$w$的偏导，进行梯度下降
$$
\frac {\partial L (w)}{\partial w} = 
\frac {\partial (w^TX^TXw-2w^TX^Ty-y^Ty)}{\partial w} \\
=\frac{\partial w^TX^TXw}{\partial w} - 2 *\frac{\partial w^TX^Ty}{\partial w} - 0\\
=X^TXw+X^TXw - 2X^Ty \\
= 2X^T(Xw-y)
$$

##### （2）实际应用

假设拟合一条直线$y=\hat{w}x+b$，通过$b = b - \epsilon * \frac {\partial L (w)}{\partial w}$ （这里的$\epsilon$是学习率）不断迭代，到最低点时斜率等于0，b不会继续被更新，此时得到最低值
$$
g = \frac{1}{N}\nabla _\theta \sum_{i=1}^{N}L(F(x_i,\theta), y_i)
$$
解决一次取N个数据量太大带来的内存开销和迭代速度的问题，改成每次随机选取m个不相同的数据进行计算，这种改进的方法叫**随机梯度下降**
$$
g = \frac{1}{m}\nabla _\theta \sum_{i=1}^{m}L(F(x_i,\theta), y_i)
$$

###### ①动量随机梯度下降

用$\alpha$控制历史动量，使梯度下降不会在一个位置反复波动而是会下降
$$
v←\alpha v - \epsilon g
$$

###### ②自动调节

保证学习率在梯度波动小的时候学习率下降慢，在梯度波动大的时候下降快一些，所以设定一个$r$来自动调整学习率

* AdaGrad

$$
r←r+g^2 \\
\theta←\theta-\frac{\epsilon}{\sqrt r + \delta}g
$$

* RMSProp

$$
r← \rho r + (1-\rho)g^2 \\
\theta←\theta-\frac{\epsilon}{\sqrt r + \delta}g
$$

$\delta$是一个小量，用来稳定数值，防止分母为0

###### ③动量+自动

$$
g = \frac{1}{m}\nabla _\theta \sum_{i=1}^{m}L(F(x_i,\theta), y_i) \\
s ← \rho _1 s + (1-\rho_2)g^2 \\
r ← \rho _1 r + (1-\rho_2)g^2
$$

$s$是自适应动量，还是通过$r$来实现自动调节
$$
\hat{s} ← \frac {s}{1-\rho_1^t} \\
\hat{r} ← \frac {r}{1-\rho_2^t}
$$
优化为
$$
\theta←\theta-\frac{\epsilon\hat{s}}{\sqrt {\hat{r}} + \delta}g
$$

##### ==（3）实际代码==

~~~python
def fit_gradient_descent(self, X, y, learning_rate=0.0001, n_iterations=100000):
        # 梯度下降
        n_samples, n_features = X.shape
        self.mdl_ = np.zeros(n_features)
        for k in range(n_iterations):
            # 梯度裁剪
            # clip_threshold = 0.5
            gradients = -(2 / n_samples) * X.T @ (y - X @ self.mdl_)
            # if clip_threshold:
               # gradients = np.clip(gradients, -clip_threshold, clip_threshold)
            self.mdl_ -= learning_rate * gradients
~~~

1. 我们的目标是最小化损失函数，通常选择的是平方误差损失函数（Mean Squared Error, MSE）：
   $$
   \text{MSE}(\theta) = \frac{1}{n} \sum_{i=1}^{n} (y_i - X_i \cdot \theta)^2
   $$
   其中，$X$ 是输入特征矩阵，$\theta$ 是模型参数向量，$y$ 是目标值向量，$n$ 是样本数量。

   求解梯度的具体计算：
   $$
   \nabla_{\theta} \text{MSE}(\theta) = -\frac{2}{n} X^T \cdot (y - X \cdot \theta)
   $$
   
2. 梯度下降算法的核心思想是通过计算损失函数对模型参数的梯度来更新参数，使得损失函数逐步减小。梯度下降的更新公式为：
   $$
   \theta = \theta - \text{learning_rate} \cdot \nabla_{\theta} \text{MSE}(\theta)
   $$
   其中，$\nabla_{\theta} \text{MSE}(\theta)$ 表示损失函数关于模型参数的梯度，$\text{learning_rate}$ 是学习率，控制参数更新的步长。

3. 在每次更新参数时，可以选择对梯度进行裁剪（gradient clipping），以防止梯度爆炸的问题。具体来说，就是设置一个阈值，如果梯度的绝对值超过了该阈值，就将其裁剪到该阈值内，以稳定优化过程。

4. 通过不断地迭代更新参数，直到达到指定的迭代次数或者损失函数收敛，即参数变化很小，可以得到最优的模型参数。
